# тестировал локально через kind (kubernetes in docker)

# запуск        kubectl apply -f ./kubernetes-task.yaml
# остановка     kubectl delete deployments.apps task-deployment
# просмотр      kubectl get pods

apiVersion: apps/v1
kind: Deployment
metadata:
  name: task-deployment
spec:
  # Если нагрузка стабильная в течении суток, и 3-х подов хватает, можно завести сразу 3, для стабильной работы со старта
  replicas: 3 
  template:
    metadata:
      labels:
        app: task-app
    spec:
      # Запуск на разных нодах
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - task-deployment
                topologyKey: "kubernetes.io/hostname"
      containers:
        - name: task-container
          image: python:bullseye # Рандомный образ, питоновский http сервер (https://habr.com/ru/articles/752586/)
          command: ["python3", "-m", "http.server", "8080"]
          name: task-deployment
          ports:
            - containerPort: 8080
          resources:
            limits:
              cpu: "1"
              # 64мб(50%) для стабильности в запас. Большая нагрузка на процессор зачастую увеличивает потребление памяти. Это зависит от приложения.
              # В случае вебсервера - Большая нагрузка зачастую от большого количества запросов, а запросы и трафик должны гдето хранится
              memory: "196Mi" 
            requests:
              cpu: "0.1"
              memory: "128Mi" 
          # Проверка пода на готовность принимать трафик
          readinessProbe:      
            failureThreshold: 3      
            httpGet:                 
              path: /
              port: 8080
            periodSeconds: 10       
            successThreshold: 1     
            timeoutSeconds: 1    
          # Проверка пульса пода, при провале - рестарт пода   
          livenessProbe:      
            failureThreshold: 3
            httpGet:               
              path: /
              port: 8080
            periodSeconds: 10       
            successThreshold: 1     
            timeoutSeconds: 1
            initialDelaySeconds: 10
      # всегда рестартить под  
      restartPolicy: Always   
  selector:
    matchLabels:
      app: task-app

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: task-deployment
spec:

  # Что скейлить:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: task-deployment

  # Нижний и верхний предел количества реплик
  minReplicas: 1
  # "3 пода справляются с нагрузкой". Раз в кластере 5 нод, можно было бы юзать все при больших нагрузах, однако нагрузка стабильная, значит не требуется
  maxReplicas: 3 

  # Метрики для определения потребности в масштабировании
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 75
  - type: Resource
    resource:
      name: memory
      target:
        type: AverageValue
        averageValue: 400Mi